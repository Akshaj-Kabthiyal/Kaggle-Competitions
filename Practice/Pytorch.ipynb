{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a65d70b",
   "metadata": {},
   "source": [
    "# Important Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c59bc",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e64a4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[ 3.,  6.],\n",
      "        [ 9., 12.]])\n",
      "tensor(2.5000)\n",
      "tensor([[1., 3.],\n",
      "        [2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Create a tensor\n",
    "x = torch.tensor([[1., 2.], [3., 4.]])\n",
    "\n",
    "# Basic operations\n",
    "print(x + 2)\n",
    "print(x * 3)\n",
    "print(x.mean())\n",
    "print(x.T)  # transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37fa3c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[-0.1989,  0.3315, -0.3665],\n",
      "        [-1.4019, -0.6886, -1.0394],\n",
      "        [ 0.8571, -0.3226,  0.4708]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.zeros(3,3))\n",
    "print(torch.ones(2,4))\n",
    "print(torch.randn(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b8021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# “Track all operations on this tensor so we can compute derivatives later.”\n",
    "x = torch.tensor([[2., 3.]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0039001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n",
      "Gradient of x: tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Step 1: Create tensor with grad tracking\n",
    "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "\n",
    "# Step 2: Do some operations\n",
    "y = x ** 2 + 3\n",
    "z = y.mean()  # scalar output required for backward()\n",
    "\n",
    "# Step 3: Backpropagate\n",
    "z.backward()\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"Gradient of x:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c32bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you’re just doing inference or want to freeze weights:\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x * 2  # no gradient tracking\n",
    "    \n",
    "x.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2135fb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Run on GPU if available:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = x.to(device)\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf86e297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 1.0000],\n",
       "        [1.5000, 2.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534971d4",
   "metadata": {},
   "source": [
    "# Torch NN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8003f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: Parameter containing:\n",
      "tensor([[ 0.4852,  0.5576, -0.1539]], requires_grad=True)\n",
      "Bias:   Parameter containing:\n",
      "tensor([-0.0402], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# pretend we have 4 samples, each with 3 input features\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [0.5, 1.0, 1.5],\n",
    "                  [2.0, 0.5, 1.0],\n",
    "                  [1.5, 1.5, 0.5]])\n",
    "\n",
    "# pretend the true outputs we want to learn are single numbers (regression)\n",
    "y_true = torch.tensor([[10.0],\n",
    "                       [ 5.0],\n",
    "                       [ 7.0],\n",
    "                       [ 8.0]])\n",
    "\n",
    "\n",
    "\n",
    "layer = nn.Linear(in_features=3, out_features=1)\n",
    "\n",
    "print(\"Weight:\", layer.weight)\n",
    "print(\"Bias:  \", layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1681abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b46215a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0985],\n",
       "        [0.5292],\n",
       "        [1.0550],\n",
       "        [1.4470]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99f308a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of first layer:\n",
      " tensor([[-0.7622],\n",
      "        [-0.6308],\n",
      "        [-1.5764],\n",
      "        [-0.7531]], grad_fn=<AddmmBackward0>)\n",
      "Output of second layer:\n",
      " tensor([[0.3735],\n",
      "        [0.2970],\n",
      "        [0.8474],\n",
      "        [0.3682]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Two layers\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [0.5, 1.0, 1.5],\n",
    "                  [2.0, 0.5, 1.0],\n",
    "                  [1.5, 1.5, 0.5]])\n",
    "\n",
    "layer1 = nn.Linear(3, 1)\n",
    "out1 = layer1(x)\n",
    "print(\"Output of first layer:\\n\", out1)\n",
    "\n",
    "layer2 = nn.Linear(1, 1)     # in_features=1 because layer1 outputs 1 value\n",
    "out2 = layer2(out1)\n",
    "print(\"Output of second layer:\\n\", out2)\n",
    "#if you multiply two linear transformations back-to-back, you just get one larger linear transformation:\n",
    "# So we need to add nonlinearity behind them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7377aa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after ReLU + second layer:\n",
      " tensor([[-0.0702],\n",
      "        [-0.0702],\n",
      "        [-0.0702],\n",
      "        [-0.0702]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "relu = nn.ReLU()\n",
    "\n",
    "h = layer1(x)       # linear transform\n",
    "h = relu(h)         # non-linear activation\n",
    "out = layer2(h)     # second linear transform\n",
    "\n",
    "print(\"Output after ReLU + second layer:\\n\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3219a8d4",
   "metadata": {},
   "source": [
    "# Backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dc6f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# input: 3 features -> hidden: 2 neurons -> output: 1 value\n",
    "layer1 = nn.Linear(3, 2)\n",
    "relu = nn.ReLU()\n",
    "layer2 = nn.Linear(2, 1)\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "y_true = torch.tensor([[10.0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd16819d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([[-0.2039]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h = layer1(x)        # linear transform\n",
    "h_relu = relu(h)     # activation\n",
    "y_pred = layer2(h_relu)  # final output\n",
    "print(\"Predicted:\", y_pred)\n",
    "\n",
    "#x → layer1(weight,bias) → h → ReLU → h_relu → layer2(weight,bias) → y_pred\n",
    "# every arrow is an operation PyTorch can differentiate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3f7d015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 104.1186294555664\n"
     ]
    }
   ],
   "source": [
    "# compute the loss (how wrong we were)\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(y_pred, y_true)\n",
    "print(\"Loss:\", loss.item())\n",
    "#this is a single scalar (mean squared error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df7a82a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "layer1.zero_grad()\n",
    "layer2.zero_grad()\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21018dc1",
   "metadata": {},
   "source": [
    "\n",
    "### 1. What happens in training\n",
    "\n",
    "Training a neural network follows this cycle:\n",
    "\n",
    "1. **Forward pass** – Pass the input `x` through the network to get predictions.\n",
    "2. **Loss computation** – Compare the predictions with the true labels using a loss function (for example, MSELoss).\n",
    "3. **Backward pass** – Call `loss.backward()` to compute gradients for every weight and bias.\n",
    "4. **Weight update** – Call `optimizer.step()` to adjust weights using those gradients.\n",
    "5. **Reset gradients** – Call `optimizer.zero_grad()` to clear gradients before the next backward pass.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What `nn.Linear` does\n",
    "\n",
    "When you create a layer like this:\n",
    "\n",
    "```python\n",
    "layer = nn.Linear(in_features=3, out_features=2)\n",
    "```\n",
    "\n",
    "PyTorch automatically:\n",
    "\n",
    "* Creates **weights** of shape `[out_features, in_features]`\n",
    "* Creates **biases** of shape `[out_features]`\n",
    "* Initializes them **randomly** (small random numbers)\n",
    "* Sets `requires_grad=True` so PyTorch can track how each parameter affects the loss\n",
    "\n",
    "---\n",
    "\n",
    "### 3. What happens in the forward pass\n",
    "\n",
    "When you do:\n",
    "\n",
    "```python\n",
    "y_pred = layer(x)\n",
    "```\n",
    "\n",
    "PyTorch computes:\n",
    "[\n",
    "y = xW^T + b\n",
    "]\n",
    "\n",
    "This gives you predictions, but weights do not change yet.\n",
    "The purpose of the forward pass is to see how wrong the predictions are.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. What the loss function does\n",
    "\n",
    "The loss function compares predicted outputs with the true outputs.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(y_pred, y_true)\n",
    "```\n",
    "\n",
    "The result is a single scalar value showing how wrong the model is.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Why we use `zero_grad()`\n",
    "\n",
    "Each parameter (weights and biases) has a `.grad` attribute that stores the gradient (slope) of the loss with respect to that parameter after backpropagation.\n",
    "\n",
    "PyTorch **adds** gradients to `.grad` every time you call `.backward()`.\n",
    "This means gradients accumulate over multiple backward passes unless you clear them.\n",
    "\n",
    "To reset gradients before a new backward pass:\n",
    "\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "This ensures gradients only reflect the current forward pass.\n",
    "\n",
    "If you skip this step, gradients from previous iterations remain, and the optimizer will make updates that are too large or completely wrong.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. What happens in the backward pass\n",
    "\n",
    "When you call:\n",
    "\n",
    "```python\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "PyTorch applies the chain rule to compute the partial derivative of the loss with respect to every parameter in the model.\n",
    "\n",
    "* For each weight: `parameter.grad = ∂Loss/∂Weight`\n",
    "* For each bias: `parameter.grad = ∂Loss/∂Bias`\n",
    "\n",
    "These gradients tell the optimizer how much to change each parameter to reduce the loss.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Updating weights\n",
    "\n",
    "When you call:\n",
    "\n",
    "```python\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "PyTorch updates every parameter using its gradient:\n",
    "[\n",
    "\\text{new_weight} = \\text{old_weight} - \\text{learning_rate} \\times \\text{gradient}\n",
    "]\n",
    "\n",
    "Weights and biases change slightly to reduce the error next time.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Summary\n",
    "\n",
    "| Step | Command                            | What it does                               |\n",
    "| ---- | ---------------------------------- | ------------------------------------------ |\n",
    "| 1    | `nn.Linear()`                      | Creates and initializes weights and biases |\n",
    "| 2    | `y_pred = layer(x)`                | Forward pass, computes predictions         |\n",
    "| 3    | `loss = criterion(y_pred, y_true)` | Computes how wrong the prediction is       |\n",
    "| 4    | `optimizer.zero_grad()`            | Clears old gradients                       |\n",
    "| 5    | `loss.backward()`                  | Computes new gradients for each parameter  |\n",
    "| 6    | `optimizer.step()`                 | Updates the weights and biases             |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Key points to remember\n",
    "\n",
    "* **Weights** and **biases** are persistent — they stay between iterations.\n",
    "* **Gradients** are temporary — they must be cleared each iteration.\n",
    "* Without `zero_grad()`, gradients accumulate and the optimizer “over-corrects.”\n",
    "* The backward pass only computes gradients; the optimizer applies them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7ee4e",
   "metadata": {},
   "source": [
    "### 1. What an **epoch** means\n",
    "\n",
    "An **epoch** = one complete pass of your entire training data through the network.\n",
    "If you train for 10 epochs, the model will see every training example 10 times.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What happens inside each epoch\n",
    "\n",
    "For every epoch, we do the same sequence of steps:\n",
    "\n",
    "1. **Forward pass**\n",
    "   Pass the current batch of inputs through the model to get predictions.\n",
    "\n",
    "2. **Compute loss**\n",
    "   Compare predictions to true outputs using a loss function.\n",
    "\n",
    "3. **Reset gradients**\n",
    "   Call `optimizer.zero_grad()` to clear old gradients.\n",
    "\n",
    "4. **Backward pass**\n",
    "   Call `loss.backward()` to compute gradients for all weights and biases.\n",
    "\n",
    "5. **Update weights**\n",
    "   Call `optimizer.step()` to adjust the parameters based on gradients.\n",
    "\n",
    "Then move on to the next batch of data (if you’re using mini-batches).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Code outline\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        y_pred = model(batch_x)                 # 1. forward pass\n",
    "        loss = criterion(y_pred, batch_y)       # 2. compute loss\n",
    "        optimizer.zero_grad()                   # 3. clear old gradients\n",
    "        loss.backward()                         # 4. backpropagation\n",
    "        optimizer.step()                        # 5. update weights\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "If you’re not using mini-batches, just drop the inner loop and run those same five lines on the whole dataset each epoch.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Why we repeat it\n",
    "\n",
    "Every time the model goes through the data:\n",
    "\n",
    "* The gradients point it a little closer to the right direction.\n",
    "* The weights move slightly toward values that reduce the loss.\n",
    "* The loss usually decreases with each epoch until the model converges or stops improving.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Summary table\n",
    "\n",
    "| Step               | Description             | Happens every epoch? |\n",
    "| ------------------ | ----------------------- | -------------------- |\n",
    "| Initialize weights | Random once             | No                   |\n",
    "| Forward pass       | Compute predictions     | Yes                  |\n",
    "| Compute loss       | Compare to true outputs | Yes                  |\n",
    "| Zero gradients     | Reset `.grad`           | Yes                  |\n",
    "| Backward pass      | Compute gradients       | Yes                  |\n",
    "| Optimizer step     | Update weights          | Yes                  |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
